# Trading Model Repository

Shared data models for the trading platform.

## Overview

- **Schema format**: Avro Schema (`.avsc`) - single source of truth
- **Generated code**: Python classes (Pydantic) via `dc-avro` CLI
- **Kafka payload**: JSON (key and value) - Avro binary planned for future

## GitHub Actions

| Workflow | Trigger | Purpose |
|----------|---------|---------|
| `generate-models-on-pr.yml` | Pull Request | Detects `.avsc` changes → generates Python models → auto-commits to PR |
| `build-and-push.yml` | Tag `v*.*.*` | Builds package from pre-generated models → publishes to GitHub Packages |

**Install published package:**
```bash
# Install specific version from GitHub Releases
pip install https://github.com/trading-cz/model/releases/download/v0.0.5/trading_model-0.0.5-py3-none-any.whl

# Or install latest from git
pip install git+https://github.com/trading-cz/model.git@main
```

**⚠️ Upgrading to a new version:**
```bash
# Always uninstall first to avoid stale cached files
pip uninstall trading-model -y
# On Windows, also remove the cz folder manually if needed:
# Remove-Item -Recurse -Force "$env:USERPROFILE\AppData\Roaming\Python\Python312\site-packages\cz"

# Then install the new version
pip install https://github.com/trading-cz/model/releases/download/vX.X.X/trading_model-X.X.X-py3-none-any.whl --no-cache-dir
```

## Repository Structure

```
schemas/                          # Avro schemas (source of truth)
├── kafka/                        # Kafka message schemas
│   ├── raw_signal/               # Trading signals from strategies
│   ├── market_stock_quote/       # Stock quotes from Alpaca
│   └── market_stock_trade/       # Stock trades from Alpaca
└── <future-system>/

src/                              # Auto-generated (never edit manually!)
└── cz/trading/model/kafka/...
```

## Available Schemas

| Schema | Topic | Description |
|--------|-------|-------------|
| `raw_signal` | `signal.raw` | Trading signals produced by strategies |
| `market_stock_quote` | `market.stock.quotes` | Stock quotes from Alpaca (ingestion-alpaca) |
| `market_stock_trade` | `market.stock.trades` | Stock trades from Alpaca (ingestion-alpaca) |

## Developer Workflow

### 1. Design a new schema

Create Avro schemas in `schemas/kafka/<topic-name>/`:

```bash
schemas/kafka/market-data/key.avsc
schemas/kafka/market-data/value.avsc
```

### 2. Create Pull Request

GitHub Actions (`generate-models-on-pr.yml`) will automatically:
- Detect if any `.avsc` files changed
- Validate schemas (`dc-avro lint`)
- Generate Python models and commit them to your PR

### 3. Release

Create a git tag:

```bash
git tag v0.0.1
git push origin v0.0.1
```

GitHub Actions (`build-and-push.yml`) will:
- Verify generated models exist
- Build Python package
- Publish to GitHub Packages

### 4. Consume in other repositories

```toml
# pyproject.toml
dependencies = ["trading-model>=0.0.1"]
```

```python
from cz.trading.model.kafka.market_data import MarketDataKey, MarketDataValue

# Serialize to JSON for Kafka
key = MarketDataKey(message_id="...", tracking_id="...", ...)
value = MarketDataValue(symbol="AAPL", price=150.0, ...)

producer.send(
    topic="market-data",
    key=key.model_dump_json().encode(),
    value=value.model_dump_json().encode(),
)

# Deserialize from Kafka JSON
key = MarketDataKey.model_validate_json(msg.key)
value = MarketDataValue.model_validate_json(msg.value)
```

## CLI Tool: dc-avro

Generate Python classes from Avro schemas using [dataclasses-avroschema](https://github.com/marcosschroh/dataclasses-avroschema).

### Installation

```bash
pip install "dataclasses-avroschema[cli]"
```

### Commands

```bash
# Generate Pydantic model from schema
dc-avro generate-model --path schemas/kafka/<topic>/key.avsc --model-type pydantic

# Validate schemas
dc-avro lint schemas/**/*.avsc

# Compare schema versions (for evolution)
dc-avro schema-diff --source-path v1.avsc --target-path v2.avsc
```

### Model Types

| Type | Description |
|------|-------------|
| `dataclass` | Python dataclass with `AvroModel` (default) |
| `pydantic` | Pydantic `BaseModel` - **recommended for JSON** |
| `avrodantic` | Pydantic + Avro helpers - for future Avro binary |

## Local Validation

Before pushing schema changes, verify locally:

```bash
# 1. Install CLI (once)
pip install "dataclasses-avroschema[cli]"

# 2. Validate schema syntax
dc-avro lint schemas/kafka/<topic>/key.avsc
dc-avro lint schemas/kafka/<topic>/value.avsc

# 3. Preview generated Python (stdout only - don't commit manually!)
dc-avro generate-model --path schemas/kafka/<topic>/value.avsc --model-type pydantic
```

**Note**: Never manually create files in `src/` - they are auto-generated by GitHub Actions on PR.

## Example Schema

### Key Schema (`schemas/kafka/common/key.avsc`)

```json
{
  "type": "record",
  "name": "CommonKey",
  "namespace": "cz.trading.model.kafka.common",
  "doc": "Standard Kafka message key with envelope metadata",
  "fields": [
    {
      "name": "message_id",
      "type": { "type": "string", "logicalType": "uuid" },
      "doc": "UUID v4: Unique identifier for this message"
    },
    {
      "name": "tracking_id",
      "type": "string",
      "doc": "Correlation ID that travels with the trade"
    },
    {
      "name": "timestamp_utc",
      "type": { "type": "long", "logicalType": "timestamp-millis" },
      "doc": "Message generation time in UTC as epoch millis"
    },
    {
      "name": "system_id",
      "type": "string",
      "doc": "Source system identifier"
    },
    {
      "name": "version",
      "type": "string",
      "default": "1.0",
      "doc": "Schema version string"
    }
  ]
}
```

## Future: Avro Binary Serialization

Currently using JSON for Kafka payloads. To migrate to Avro binary:

1. Change `--model-type pydantic` to `--model-type avrodantic`
2. Use `serialize()` / `deserialize()` methods instead of `model_dump_json()`

```python
# Future Avro binary usage
key = CommonKey(...)
binary = key.serialize()  # Avro binary
key = CommonKey.deserialize(binary)
```
